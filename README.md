# harry_potter_bot

## author 

Dan Steinbrook: steinbrookdaniel at gmail

## screencap

![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExYjkwYjM5MTNhNTA1YjQ1ODJjZmU0OTM2Mjc0M2U4YzM5ZWE3OGJhZSZlcD12MV9pbnRlcm5hbF9naWZzX2dpZklkJmN0PWc/xqI3R0AXyUUan5cTVR/giphy.gif)

## about

This is a repo for a demo version of an app that allows the user to converse with and ask questions to a bot representing a character from the Harry Potter universe. The app uses the ChatGPT API (gpt-3.5-turbo model) from Openai. It also uses a vector database creating using Openai's text-embedding-data-002 model to provide the characters' memories (see ```src/server/data``` for the data files). 

```Ronald_Weasley.json``` is scraped from https://harrypotter.fandom.com/wiki/Ronald_Weasley' then chunked and embedded using the ```scrape.py``` script and ```Amara_Nightengale.json``` represents a made up character from the Harry Potter universe with memories generated by ChatGPT then embedded. 

Could be extended to create videogame NPC's with dynamically added memories. 

The ChatGPT API is likely using its built-in knowledge of Ron Weasley in addition to the provided data from the Harry Potter wiki, but the Amara Nightengale character provides a control where all the bot's knowledge of the character is coming from the vector database because no record of the character exists in ChatGPT's training data. 

The app gives the ChatGPT API access to the character's knowledge about themselves by using the user's prompt as a query for semantic search in the vector database to retrieve the portion of the datafile relevant to the user's query, which is then inserted into the prompt. 

Two methods are provided for vector database search: cosine similarity and training a Support Vector Machine classifier (see ```src/server/server.py```).

The demo version uses the ChatGPT API, see ```README-TRAINING_OUTLINE.md``` for an outline of the process for training a custom chat language model.

## usage

to run on port 8080 in docker container (more reliable): 
```
docker build -t hp-bot .
```

```
docker run -it -p 8080:8080 -e OPENAI_API_KEY=<key_here> hp-bot
```
alternative to run dev environment on port 8080:

```
export OPENAI_API_KEY=<your key here>
pip install -r requirements.txt
npm install
npm run dev
```

## potential improvements/added features

This demo implementation only adds the external memory context to the most recent message in the query to the ChatGPT API.
As a result the chat performs well in shorter conversations, but sometimes stops using its external memories as effectively in longer conversations. This could be improved by using e.g. the Chain class from the LangChain library which provides a method for integrating a chat history memory object with externally retrieved memories, see https://python.langchain.com/en/latest/_modules/langchain/chains/conversational_retrieval/base.html#ConversationalRetrievalChain. 

Also for scaling the app for production you could use a vector database such as https://www.pinecone.io/ to store embeddings more efficiently as opposed to json files.



